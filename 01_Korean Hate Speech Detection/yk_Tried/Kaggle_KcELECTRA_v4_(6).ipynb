{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w29U8Jm281fo",
        "outputId": "7a675a69-98a8-4374-edb9-bc5a0b9c9bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "rlVnYRkQ66-s",
        "outputId": "895d71df-0509-482b-cb73-a2fe1e9bac2d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidIndexError",
          "evalue": "Reindexing only valid with uniquely valued Index objects",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-35a2a463cbb5>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# 데이터프레임 병합 시 중복된 열이 없도록 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df_augmented\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    393\u001b[0m     )\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m                     \u001b[0mobj_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 680\u001b[0;31m                         \u001b[0mindexers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[0;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[1;32m   3883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_as_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3885\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requires_unique_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
          ]
        }
      ],
      "source": [
        "# ### 1. 데이터 증강 (Data Augmentation)을 사용한 KcELECTRA 모델 코드\n",
        "\n",
        "# import pandas as pd\n",
        "# import random\n",
        "# from nltk.corpus import wordnet\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from transformers import ElectraForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
        "# import torch\n",
        "\n",
        "# # 텍스트 증강 함수 정의\n",
        "# def synonym_replacement(sentence):\n",
        "#     words = sentence.split()\n",
        "#     new_sentence = []\n",
        "#     for word in words:\n",
        "#         synonyms = wordnet.synsets(word)\n",
        "#         if synonyms:\n",
        "#             new_word = synonyms[0].lemmas()[0].name()\n",
        "#             new_sentence.append(new_word if new_word != word else word)\n",
        "#         else:\n",
        "#             new_sentence.append(word)\n",
        "#     return ' '.join(new_sentence)\n",
        "\n",
        "# # 학습 데이터 증강\n",
        "# train_df = pd.read_csv('train.hate.csv')\n",
        "# dev_df = pd.read_csv('dev.hate.csv')\n",
        "\n",
        "# # 학습 데이터 증강 후 인덱스 재설정\n",
        "# train_df['comments_augmented'] = train_df['comments'].apply(synonym_replacement)\n",
        "# train_df_augmented = train_df.rename(columns={'comments_augmented': 'comments'})[['comments', 'label']].reset_index(drop=True)\n",
        "\n",
        "# # 원본 데이터프레임의 인덱스 재설정 및 중복 열 제거\n",
        "# train_df = train_df[['comments', 'label']].reset_index(drop=True)\n",
        "\n",
        "# # 데이터프레임 병합 시 중복된 열이 없도록 확인\n",
        "# train_df = pd.concat([train_df, train_df_augmented], axis=0, ignore_index=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# 학습, 검증, 테스트 데이터 로드\n",
        "train_df = pd.read_csv('train.hate.csv')  # 학습 데이터 로드\n",
        "dev_df = pd.read_csv('dev.hate.csv')      # 검증 데이터 로드\n",
        "test_df = pd.read_csv('test.hate.no_label.csv')    # 테스트 데이터 로드\n",
        "\n",
        "# 라벨링 변환 (예: 'no' -> 0, 'offensive' -> 1, 'hate' -> 2)\n",
        "label_mapping = {'no': 0, 'offensive': 1, 'hate': 2}\n",
        "train_df['label'] = train_df['label'].map(label_mapping)\n",
        "dev_df['label'] = dev_df['label'].map(label_mapping)\n",
        "\n",
        "# 결측치 확인 및 제거\n",
        "train_df = train_df.dropna(subset=['label', 'comments'])\n",
        "dev_df = dev_df.dropna(subset=['label', 'comments'])\n",
        "\n",
        "# 데이터셋 확인\n",
        "if train_df.empty:\n",
        "    raise ValueError(\"학습 데이터셋이 비어 있습니다. 데이터 파일의 내용을 확인하세요.\")\n",
        "if dev_df.empty:\n",
        "    raise ValueError(\"검증 데이터셋이 비어 있습니다. 데이터 파일의 내용을 확인하세요.\")"
      ],
      "metadata": {
        "id": "5g3XLDRL9xaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ElectraForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "\n",
        "# 학습 설정에 학습률 스케줄러 추가\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# 학습률 스케줄러 생성\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=500, num_training_steps=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "TgX6bz1D7VxQ",
        "outputId": "c71999e8-1e4b-4b75-cb12-1337c8dbb0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "--load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: epoch\n- Save strategy: steps",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-1d6b1dbcdb77>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# 학습 설정에 학습률 스케줄러 추가\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./results'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, ad...\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_best_model_at_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_strategy\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_strategy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1594\u001b[0m                     \u001b[0;34m\"--load_best_model_at_end requires the save and eval strategy to match, but found\\n- Evaluation \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m                     \u001b[0;34mf\"strategy: {self.eval_strategy}\\n- Save strategy: {self.save_strategy}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: --load_best_model_at_end requires the save and eval strategy to match, but found\n- Evaluation strategy: epoch\n- Save strategy: steps"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "\n",
        "# 조기 종료 콜백 정의\n",
        "class EarlyStoppingCallback(TrainerCallback):\n",
        "    def __init__(self, patience=3):\n",
        "        self.patience = patience\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.counter = 0\n",
        "\n",
        "    def on_evaluate(self, args, state, control, **kwargs):\n",
        "        eval_metric = state.log_history[-1]['eval_loss']\n",
        "        if self.best_score is None or eval_metric < self.best_score:\n",
        "            self.best_score = eval_metric\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                control.should_training_stop = True\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    callbacks=[EarlyStoppingCallback(patience=3)]\n",
        ")"
      ],
      "metadata": {
        "id": "74uSY2MV7X1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ElectraForSequenceClassification\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "# 여러 모델 훈련 후 예측 결합\n",
        "models = [ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\", num_labels=3) for _ in range(3)]\n",
        "trainers = []\n",
        "\n",
        "for i, model in tqdm(enumerate(models)):\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_{i}',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        learning_rate=5e-5,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=True\n",
        "    )\n",
        "    trainers.append(Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset\n",
        "    ))\n",
        "    trainers[-1].train()\n",
        "\n",
        "# 예측 결합 코드\n",
        "final_predictions = sum(trainer.predict(test_dataset).predictions for trainer in trainers) / len(trainers)\n",
        "final_preds = torch.argmax(torch.tensor(final_predictions), axis=1)"
      ],
      "metadata": {
        "id": "CDI5Qs3e7hXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ElectraConfig, ElectraForSequenceClassification\n",
        "\n",
        "# 모델 설정 변경\n",
        "config = ElectraConfig.from_pretrained(\"beomi/KcELECTRA-base\")\n",
        "config.hidden_dropout_prob = 0.3  # Dropout 비율 조정\n",
        "model = ElectraForSequenceClassification.from_pretrained(\"beomi/KcELECTRA-base\", config=config, num_labels=3)"
      ],
      "metadata": {
        "id": "hoGtKBlx7pUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.1,  # 높은 weight decay 설정\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "\n",
        "# 모델 훈련\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "fnm-IgZl7sXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 훈련 후 평가 지표 계산 및 혼동 행렬 생성\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 평가 및 예측\n",
        "predictions = trainer.predict(val_dataset)\n",
        "y_preds = torch.argmax(torch.tensor(predictions.predictions), axis=1)\n",
        "y_true = val_labels.numpy()\n",
        "\n",
        "# 평가 지표 출력\n",
        "print(classification_report(y_true, y_preds, target_names=['no', 'offensive', 'hate']))"
      ],
      "metadata": {
        "id": "SOwkIzIr8Iw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 혼동 행렬 그리기\n",
        "conf_matrix = confusion_matrix(y_true, y_preds)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['no', 'offensive', 'hate'], yticklabels=['no', 'offensive', 'hate'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HehKxLoY8Vtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 및 토크나이저 저장\n",
        "model.save_pretrained('./saved_model_v4')\n",
        "tokenizer.save_pretrained('./saved_model_v4')"
      ],
      "metadata": {
        "id": "bc0bqDug8W-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 데이터 예측 및 CSV 저장\n",
        "test_encodings = encode_data(test_df)\n",
        "test_dataset = CustomDataset(test_encodings)\n",
        "test_predictions = trainer.predict(test_dataset)\n",
        "test_preds = torch.argmax(torch.tensor(test_predictions.predictions), axis=1)\n",
        "\n",
        "output_df = test_df.copy()\n",
        "output_df['predicted_label'] = test_preds.numpy()\n",
        "output_df.to_csv('test.hate.predicted_KcELECTRA_v4.csv', index=False)\n",
        "print(\"테스트 데이터의 예측 결과가 test.hate.predicted_KcELECTRA_v4.csv 파일에 저장되었습니다.\")\n"
      ],
      "metadata": {
        "id": "TyrfPXIU8aqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Colab에서 모델 압축 및 다운로드\n",
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "shutil.make_archive('saved_model_v4', 'zip', './saved_model_v4')\n",
        "files.download('saved_model_v4.zip')"
      ],
      "metadata": {
        "id": "kUODBd6F8Y6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1YBpU9ar8me6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}